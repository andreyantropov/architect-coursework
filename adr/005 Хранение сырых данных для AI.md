# ADR 005: Хранение сырых данных для AI

**Дата:** 2025-04-05  
**Участники:** AI/ML Engineer, Data Architect, DevOps Lead

## Статус

Принято  
_2025-04-05_

## Контекст

Требуется определить способ хранения и управления сырыми данными для обучения AI-моделей с возможностью версионирования и обработки больших объемов.

## Варианты решения

1. **S3 + Delta Lake**
   - ✅ _Плюсы_: Масштабируемость, поддержка ACID-транзакций, совместимость с Spark
   - ❌ _Минусы_: Нужно управлять метаданными отдельно
2. **Lakehouse (Databricks)**
   - ✅ _Плюсы_: Единая платформа для хранения и вычислений
   - ❌ _Минусы_: Vendor lock-in, высокая стоимость
3. **HDFS + Hadoop Cluster**
   - ✅ _Плюсы_: Полный контроль
   - ❌ _Минусы_: Высокие операционные затраты, сложность масштабирования

## Решение

**Выбранный вариант:** S3 + Delta Lake  
**Обоснование:** Открытые стандарты, гибкость, низкие затраты на хранение, простота интеграции с MLflow.

## Последствия

- **Положительные:** Поддержка versioning, audit trail, легкая интеграция с pipeline
- **Отрицательные:** Требуется дополнительная оркестровка метаданных
- **Риски:** Возможна деградация производительности при неоптимальном партиционировании
